{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bXfZXsNhy08r"
   },
   "source": [
    "# Using VideoGPT for all-sky video\n",
    "This is a notebook demonstrating how to use VideoGPT on all-sky video."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ponLMda7zBmF"
   },
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6G1lfDiwycLl",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import Video\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import animation\n",
    "from IPython.display import HTML\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from torchvision.io import read_video, read_video_timestamps\n",
    "\n",
    "from videogpt import download, load_vqvae, load_videogpt\n",
    "from videogpt.data import preprocess\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "VIDEOS = {\n",
    "    'breakdancing': '1OZBnG235-J9LgB_qHv-waHZ4tjofiDgj',\n",
    "    'bear': '16nIaqq2vbPh-WMo_7hs9feVSe0jWVXLF',\n",
    "    'jaywalking': '1UxKCVrbyXhvMz_H7dI4w5hjPpRGCAApy',\n",
    "    'cartoon': '1ONcTMSEuGuLYIDbX-KeFqd390vbTIH9d'\n",
    "}\n",
    "\n",
    "ROOT = 'pretrained_models'\n",
    "\n",
    "models = ['bair_stride4x2x2', 'ucf101_stride4x4x4', 'kinetics_stride4x4x4', 'kinetics_stride2x4x4']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u_RyF9AU2tzK"
   },
   "source": [
    "## Downloading a Pretrained VQ-VAE\n",
    "There are four pretrained models available with VideoGPT: `bair_stride4x2x2`, `ucf101_stride4x4x4`, `kinetics_stride4x4x4`, and `kinetics_stride2x4x4`. BAIR was trained on 64 x 64 video, and the rest on 128 x 128. The `stride` component represents the THW downsampling the VQ-VAE performs on the video tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4t5fEML30L3f",
    "outputId": "af31fe5a-d0c7-4648-9107-6c36c0ee5199"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "vqvae = load_vqvae('kinetics_stride2x4x4', device=device, root=ROOT).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-QMgNCPo3jQg"
   },
   "source": [
    "## Video Loading and Preprocessing\n",
    "The code below downloads, loads, and preprocesses a given `mp4` file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X_FYfsIf2kwU",
    "outputId": "e3d669fb-8998-4f4f-efea-f8d79e1fc587"
   },
   "outputs": [],
   "source": [
    "video_name = 'bear'\n",
    "video_filename = download(VIDEOS[video_name], f'{video_name}.mp4')\n",
    "Video(video_filename, embed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X_FYfsIf2kwU",
    "outputId": "e3d669fb-8998-4f4f-efea-f8d79e1fc587"
   },
   "outputs": [],
   "source": [
    "# `resolution` must be divisible by the encoder image stride\n",
    "# `sequence_length` must be divisible by the encoder temporal stride\n",
    "resolution, sequence_length = vqvae.args.resolution, 16\n",
    "\n",
    "\n",
    "pts = read_video_timestamps(video_filename, pts_unit='sec')[0]\n",
    "video = read_video(video_filename, pts_unit='sec', start_pts=pts[0], end_pts=pts[sequence_length - 1])[0]\n",
    "video = preprocess(video, resolution, sequence_length).unsqueeze(0).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "video.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rA3R-ZOi3uri"
   },
   "source": [
    "## VQ-VAE Encoding and Decoding\n",
    "Now, we can encode the video through the `encode` function.  \n",
    "The `encode` function also has an optional input `including_embeddings` (default `False`) which will also return the embedding versions of the encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ywTjc5wi2odm"
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    encodings = vqvae.encode(video)\n",
    "    video_recon = vqvae.decode(encodings)\n",
    "    video_recon = torch.clamp(video_recon, -0.5, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tcyzbBVX4J-d"
   },
   "source": [
    "## Visualizing Reconstructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 309
    },
    "id": "Y2t-dwme2qN1",
    "outputId": "aee70996-4608-4a19-cac6-a098da960588"
   },
   "outputs": [],
   "source": [
    "videos = torch.cat((video, video_recon), dim=-1)\n",
    "videos = videos[0].permute(1, 2, 3, 0) # CTHW -> THWC\n",
    "videos = ((videos + 0.5) * 255).cpu().numpy().astype('uint8')\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.title('real (left), reconstruction (right)')\n",
    "plt.axis('off')\n",
    "im = plt.imshow(videos[0, :, :, :])\n",
    "plt.close()\n",
    "\n",
    "def init():\n",
    "    im.set_data(videos[0, :, :, :])\n",
    "\n",
    "def animate(i):\n",
    "    im.set_data(videos[i, :, :, :])\n",
    "    return im\n",
    "\n",
    "anim = animation.FuncAnimation(fig, animate, init_func=init, frames=videos.shape[0], interval=200) # 200ms = 5 fps\n",
    "HTML(anim.to_html5_video())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qz3EFC-ZXL7z"
   },
   "source": [
    "# Using Pretrained VideoGPT Models\n",
    "\n",
    "The current available model to download is `ucf101`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "95FevjM0XG_y",
    "outputId": "6a8f110f-25ec-47e5-9e63-044f0e7b2486"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda')\n",
    "gpt = load_videogpt('ucf101_uncond_gpt', device=device).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TMlpXebRY3P5"
   },
   "source": [
    "`VideoGPT.sample` method returns generated samples of shape BCTHW in the range [0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3o2aPtJQXnjX",
    "outputId": "ced778c8-087a-47b7-d974-b952b76f6b39"
   },
   "outputs": [],
   "source": [
    "samples = gpt.sample(16) # unconditional model does not require batch input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 309
    },
    "id": "WZxFIFYzY_Kj",
    "outputId": "63d60c34-3a82-4375-e4bb-c56b280ec13d"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "b, c, t, h, w = samples.shape\n",
    "samples = samples.permute(0, 2, 3, 4, 1)\n",
    "samples = (samples.cpu().numpy() * 255).astype('uint8')\n",
    "\n",
    "video = np.zeros((t, (1 + h) * 4 + 1, (1 + w) * 4 + 1, c), dtype='uint8')\n",
    "for i in range(b):\n",
    "  r, c = i // 4, i % 4\n",
    "  start_r, start_c = (1 + h) * r, (1 + w) * c\n",
    "  video[:, start_r:start_r + h, start_c:start_c + w] = samples[i]\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.title('ucf101 unconditional samples')\n",
    "plt.axis('off')\n",
    "im = plt.imshow(video[0, :, :, :])\n",
    "plt.close()\n",
    "\n",
    "def init():\n",
    "    im.set_data(video[0, :, :, :])\n",
    "\n",
    "def animate(i):\n",
    "    im.set_data(video[i, :, :, :])\n",
    "    return im\n",
    "\n",
    "anim = animation.FuncAnimation(fig, animate, init_func=init, frames=video.shape[0], interval=200) # 200ms = 5 fps\n",
    "HTML(anim.to_html5_video())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iCznGxs-lYNq"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Using_VideoGPT.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
